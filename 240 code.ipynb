{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6eb1f675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "=== Logistic Regression Experiments ===\n",
      "\n",
      "[Logistic] Training with Gradient Descent (full batch)...\n",
      "Epoch   1 | Train Loss: 0.7955 | Val Loss: 0.5887 | Val Acc: 0.7912\n",
      "Epoch  10 | Train Loss: 0.2577 | Val Loss: 0.2587 | Val Acc: 0.9231\n",
      "Epoch  20 | Train Loss: 0.1918 | Val Loss: 0.1928 | Val Acc: 0.9451\n",
      "Epoch  30 | Train Loss: 0.1627 | Val Loss: 0.1615 | Val Acc: 0.9560\n",
      "Epoch  40 | Train Loss: 0.1455 | Val Loss: 0.1422 | Val Acc: 0.9560\n",
      "Epoch  50 | Train Loss: 0.1340 | Val Loss: 0.1288 | Val Acc: 0.9780\n",
      "\n",
      "[Logistic] Training with SGD (mini-batch)...\n",
      "Epoch   1 | Train Loss: 0.4473 | Val Loss: 0.3260 | Val Acc: 0.9451\n",
      "Epoch  10 | Train Loss: 0.1284 | Val Loss: 0.1196 | Val Acc: 0.9780\n",
      "Epoch  20 | Train Loss: 0.1014 | Val Loss: 0.0902 | Val Acc: 0.9670\n",
      "Epoch  30 | Train Loss: 0.0903 | Val Loss: 0.0766 | Val Acc: 0.9670\n",
      "Epoch  40 | Train Loss: 0.0837 | Val Loss: 0.0690 | Val Acc: 0.9890\n",
      "Epoch  50 | Train Loss: 0.0791 | Val Loss: 0.0633 | Val Acc: 0.9890\n",
      "\n",
      "[Logistic] Training with Nesterov Accelerated Gradient...\n",
      "Epoch   1 | Train Loss: 0.3149 | Val Loss: 0.1219 | Val Acc: 0.9560\n",
      "Epoch  10 | Train Loss: 0.0657 | Val Loss: 0.0461 | Val Acc: 0.9890\n",
      "Epoch  20 | Train Loss: 0.0594 | Val Loss: 0.0378 | Val Acc: 0.9890\n",
      "Epoch  30 | Train Loss: 0.0560 | Val Loss: 0.0357 | Val Acc: 0.9890\n",
      "Epoch  40 | Train Loss: 0.0544 | Val Loss: 0.0331 | Val Acc: 0.9890\n",
      "Epoch  50 | Train Loss: 0.0530 | Val Loss: 0.0320 | Val Acc: 0.9890\n",
      "\n",
      "[Logistic] Training with Adam...\n",
      "Epoch   1 | Train Loss: 0.3913 | Val Loss: 0.2593 | Val Acc: 0.9451\n",
      "Epoch  10 | Train Loss: 0.0923 | Val Loss: 0.0676 | Val Acc: 0.9890\n",
      "Epoch  20 | Train Loss: 0.0730 | Val Loss: 0.0476 | Val Acc: 0.9890\n",
      "Epoch  30 | Train Loss: 0.0651 | Val Loss: 0.0403 | Val Acc: 0.9890\n",
      "Epoch  40 | Train Loss: 0.0604 | Val Loss: 0.0355 | Val Acc: 0.9890\n",
      "Epoch  50 | Train Loss: 0.0579 | Val Loss: 0.0340 | Val Acc: 0.9890\n",
      "\n",
      "[Logistic] Training with SPSA (gradient-free)...\n",
      "[SPSA] Iter    1 | Train Loss: 0.5372 | Val Loss: 0.5706 | Val Acc: 0.7582\n",
      "[SPSA] Iter   10 | Train Loss: 0.3979 | Val Loss: 0.4324 | Val Acc: 0.8681\n",
      "[SPSA] Iter   20 | Train Loss: 0.3214 | Val Loss: 0.3466 | Val Acc: 0.9121\n",
      "[SPSA] Iter   30 | Train Loss: 0.2802 | Val Loss: 0.3059 | Val Acc: 0.9011\n",
      "[SPSA] Iter   40 | Train Loss: 0.2549 | Val Loss: 0.2759 | Val Acc: 0.9121\n",
      "[SPSA] Iter   50 | Train Loss: 0.2480 | Val Loss: 0.2690 | Val Acc: 0.9121\n",
      "[SPSA] Iter   60 | Train Loss: 0.2404 | Val Loss: 0.2611 | Val Acc: 0.9121\n",
      "[SPSA] Iter   70 | Train Loss: 0.2303 | Val Loss: 0.2483 | Val Acc: 0.9121\n",
      "[SPSA] Iter   80 | Train Loss: 0.2248 | Val Loss: 0.2422 | Val Acc: 0.9121\n",
      "[SPSA] Iter   90 | Train Loss: 0.2241 | Val Loss: 0.2416 | Val Acc: 0.9231\n",
      "[SPSA] Iter  100 | Train Loss: 0.2187 | Val Loss: 0.2360 | Val Acc: 0.9231\n",
      "[SPSA] Iter  110 | Train Loss: 0.2165 | Val Loss: 0.2333 | Val Acc: 0.9231\n",
      "[SPSA] Iter  120 | Train Loss: 0.2114 | Val Loss: 0.2271 | Val Acc: 0.9231\n",
      "[SPSA] Iter  130 | Train Loss: 0.2078 | Val Loss: 0.2225 | Val Acc: 0.9231\n",
      "[SPSA] Iter  140 | Train Loss: 0.2056 | Val Loss: 0.2195 | Val Acc: 0.9231\n",
      "[SPSA] Iter  150 | Train Loss: 0.2022 | Val Loss: 0.2154 | Val Acc: 0.9341\n",
      "[SPSA] Iter  160 | Train Loss: 0.1998 | Val Loss: 0.2129 | Val Acc: 0.9341\n",
      "[SPSA] Iter  170 | Train Loss: 0.1976 | Val Loss: 0.2102 | Val Acc: 0.9341\n",
      "[SPSA] Iter  180 | Train Loss: 0.1949 | Val Loss: 0.2076 | Val Acc: 0.9341\n",
      "[SPSA] Iter  190 | Train Loss: 0.1925 | Val Loss: 0.2048 | Val Acc: 0.9451\n",
      "[SPSA] Iter  200 | Train Loss: 0.1902 | Val Loss: 0.2018 | Val Acc: 0.9451\n",
      "\n",
      "[SPSA] Finished in 0.153 seconds.\n",
      "\n",
      "=== Logistic Regression Summary (Test Set) ===\n",
      "Method   |  Test Loss |  Test Acc | Time (s)\n",
      "---------------------------------------------\n",
      "GD       |     0.1794 |    0.9211 |    0.234\n",
      "SGD      |     0.1007 |    0.9737 |    0.791\n",
      "NAG      |     0.0854 |    0.9737 |    0.727\n",
      "Adam     |     0.0897 |    0.9649 |    0.853\n",
      "SPSA     |     0.2391 |    0.9211 |    0.153\n",
      "\n",
      "\n",
      "=== Small Neural Network Experiments ===\n",
      "\n",
      "[MLP] Training with Gradient Descent (full batch)...\n",
      "Epoch   1 | Train Loss: 0.7245 | Val Loss: 0.6691 | Val Acc: 0.6703\n",
      "Epoch   5 | Train Loss: 0.5828 | Val Loss: 0.5507 | Val Acc: 0.9011\n",
      "Epoch  10 | Train Loss: 0.4576 | Val Loss: 0.4384 | Val Acc: 0.9560\n",
      "Epoch  15 | Train Loss: 0.3629 | Val Loss: 0.3516 | Val Acc: 0.9670\n",
      "Epoch  20 | Train Loss: 0.2952 | Val Loss: 0.2885 | Val Acc: 0.9670\n",
      "\n",
      "[MLP] Training with SGD (mini-batch)...\n",
      "Epoch   1 | Train Loss: 0.6290 | Val Loss: 0.5446 | Val Acc: 0.9011\n",
      "Epoch   5 | Train Loss: 0.2351 | Val Loss: 0.2244 | Val Acc: 0.9560\n",
      "Epoch  10 | Train Loss: 0.1375 | Val Loss: 0.1288 | Val Acc: 0.9670\n",
      "Epoch  15 | Train Loss: 0.1036 | Val Loss: 0.0935 | Val Acc: 0.9670\n",
      "Epoch  20 | Train Loss: 0.0859 | Val Loss: 0.0745 | Val Acc: 0.9890\n",
      "\n",
      "[MLP] Training with Nesterov Accelerated Gradient...\n",
      "Epoch   1 | Train Loss: 0.4031 | Val Loss: 0.1521 | Val Acc: 0.9670\n",
      "Epoch   5 | Train Loss: 0.0571 | Val Loss: 0.0416 | Val Acc: 0.9780\n",
      "Epoch  10 | Train Loss: 0.0416 | Val Loss: 0.0327 | Val Acc: 1.0000\n",
      "Epoch  15 | Train Loss: 0.0304 | Val Loss: 0.0249 | Val Acc: 1.0000\n",
      "Epoch  20 | Train Loss: 0.0237 | Val Loss: 0.0228 | Val Acc: 1.0000\n",
      "\n",
      "[MLP] Training with Adam...\n",
      "Epoch   1 | Train Loss: 0.3656 | Val Loss: 0.1391 | Val Acc: 0.9560\n",
      "Epoch   5 | Train Loss: 0.0560 | Val Loss: 0.0306 | Val Acc: 0.9890\n",
      "Epoch  10 | Train Loss: 0.0331 | Val Loss: 0.0166 | Val Acc: 1.0000\n",
      "Epoch  15 | Train Loss: 0.0171 | Val Loss: 0.0108 | Val Acc: 1.0000\n",
      "Epoch  20 | Train Loss: 0.0096 | Val Loss: 0.0092 | Val Acc: 1.0000\n",
      "\n",
      "[MLP] Training with SPSA (gradient-free)...\n",
      "[MLP SPSA] Iter    1 | Train Loss: 0.6835 | Val Loss: 0.6677 | Val Acc: 0.6154\n",
      "[MLP SPSA] Iter   10 | Train Loss: 0.6585 | Val Loss: 0.6453 | Val Acc: 0.6813\n",
      "[MLP SPSA] Iter   20 | Train Loss: 0.6293 | Val Loss: 0.6212 | Val Acc: 0.8022\n",
      "[MLP SPSA] Iter   30 | Train Loss: 0.6046 | Val Loss: 0.6002 | Val Acc: 0.8022\n",
      "[MLP SPSA] Iter   40 | Train Loss: 0.5994 | Val Loss: 0.5946 | Val Acc: 0.7912\n",
      "[MLP SPSA] Iter   50 | Train Loss: 0.5863 | Val Loss: 0.5834 | Val Acc: 0.8242\n",
      "[MLP SPSA] Iter   60 | Train Loss: 0.5743 | Val Loss: 0.5720 | Val Acc: 0.8462\n",
      "[MLP SPSA] Iter   70 | Train Loss: 0.5560 | Val Loss: 0.5564 | Val Acc: 0.8791\n",
      "[MLP SPSA] Iter   80 | Train Loss: 0.5493 | Val Loss: 0.5506 | Val Acc: 0.9011\n",
      "[MLP SPSA] Iter   90 | Train Loss: 0.5412 | Val Loss: 0.5415 | Val Acc: 0.9011\n",
      "[MLP SPSA] Iter  100 | Train Loss: 0.5298 | Val Loss: 0.5310 | Val Acc: 0.9121\n",
      "[MLP SPSA] Iter  110 | Train Loss: 0.5263 | Val Loss: 0.5274 | Val Acc: 0.9121\n",
      "[MLP SPSA] Iter  120 | Train Loss: 0.5187 | Val Loss: 0.5211 | Val Acc: 0.9121\n",
      "[MLP SPSA] Iter  130 | Train Loss: 0.5137 | Val Loss: 0.5164 | Val Acc: 0.9231\n",
      "[MLP SPSA] Iter  140 | Train Loss: 0.5017 | Val Loss: 0.5050 | Val Acc: 0.9231\n",
      "[MLP SPSA] Iter  150 | Train Loss: 0.4924 | Val Loss: 0.4963 | Val Acc: 0.9231\n",
      "[MLP SPSA] Iter  160 | Train Loss: 0.4813 | Val Loss: 0.4866 | Val Acc: 0.9231\n",
      "[MLP SPSA] Iter  170 | Train Loss: 0.4696 | Val Loss: 0.4747 | Val Acc: 0.9341\n",
      "[MLP SPSA] Iter  180 | Train Loss: 0.4654 | Val Loss: 0.4708 | Val Acc: 0.9341\n",
      "[MLP SPSA] Iter  190 | Train Loss: 0.4600 | Val Loss: 0.4657 | Val Acc: 0.9341\n",
      "[MLP SPSA] Iter  200 | Train Loss: 0.4561 | Val Loss: 0.4621 | Val Acc: 0.9341\n",
      "\n",
      "=== MLP Summary (Test Set) ===\n",
      "Method   |  Test Loss |  Test Acc | Time (s)\n",
      "---------------------------------------------\n",
      "GD       |     0.3050 |    0.9298 |    0.081\n",
      "SGD      |     0.1197 |    0.9649 |    0.393\n",
      "NAG      |     0.1106 |    0.9561 |    0.406\n",
      "Adam     |     0.1358 |    0.9474 |    0.335\n",
      "SPSA     |     0.4755 |    0.9123 |    0.221\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "IEOR 240 Project: Comparing Optimization Methods for ML Training\n",
    "\n",
    "Methods:\n",
    "- Gradient Descent (full-batch)\n",
    "- Stochastic Gradient Descent (mini-batch)\n",
    "- Nesterov Accelerated Gradient (NAG)\n",
    "- Adam\n",
    "- SPSA (Simultaneous Perturbation Stochastic Approximation, gradient-free)\n",
    "\n",
    "Models:\n",
    "- Logistic Regression\n",
    "- Small 2-layer Neural Network\n",
    "\n",
    "Dataset:\n",
    "- Breast Cancer dataset from sklearn (binary classification, small & fast)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Data loading utilities\n",
    "# ---------------------------\n",
    "\n",
    "def load_breast_cancer_torch(test_size=0.2, random_state=0, device=\"cpu\"):\n",
    "    data = load_breast_cancer()\n",
    "    X = data.data\n",
    "    y = data.target  # 0/1\n",
    "\n",
    "    # Standardize features for stability\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    X_train_t = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
    "    y_train_t = torch.tensor(y_train, dtype=torch.long, device=device)\n",
    "    X_test_t = torch.tensor(X_test, dtype=torch.float32, device=device)\n",
    "    y_test_t = torch.tensor(y_test, dtype=torch.long, device=device)\n",
    "\n",
    "    return X_train_t, X_test_t, y_train_t, y_test_t\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Model definitions\n",
    "# ---------------------------\n",
    "\n",
    "class LogisticModel(nn.Module):\n",
    "    \"\"\"Logistic regression: linear layer + sigmoid (internally via BCEWithLogitsLoss).\"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Return logits (no sigmoid here, handled by loss)\n",
    "        return self.linear(x).squeeze(-1)\n",
    "\n",
    "\n",
    "class SmallMLP(nn.Module):\n",
    "    \"\"\"Small 2-layer neural network for binary classification.\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.act = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)  # binary logits\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x).squeeze(-1)\n",
    "        return x  # logits\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Common training utilities\n",
    "# ---------------------------\n",
    "\n",
    "def binary_accuracy_from_logits(logits, y_true):\n",
    "    \"\"\"\n",
    "    logits: tensor of shape (N,)\n",
    "    y_true: long tensor of {0,1} with shape (N,)\n",
    "    \"\"\"\n",
    "    probs = torch.sigmoid(logits)\n",
    "    y_pred = (probs >= 0.5).long()\n",
    "    acc = (y_pred == y_true).float().mean().item()\n",
    "    return acc\n",
    "\n",
    "\n",
    "def train_one_epoch(model, optimizer, loss_fn, dataloader, device=\"cpu\"):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    n_samples = 0\n",
    "\n",
    "    for xb, yb in dataloader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = loss_fn(logits, yb.float())  # BCEWithLogitsLoss expects float targets\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_size = xb.size(0)\n",
    "        total_loss += loss.item() * batch_size\n",
    "        n_samples += batch_size\n",
    "\n",
    "    return total_loss / n_samples\n",
    "\n",
    "\n",
    "def evaluate_model(model, loss_fn, X, y, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(X.to(device))\n",
    "        loss = loss_fn(logits, y.to(device).float()).item()\n",
    "        acc = binary_accuracy_from_logits(logits, y.to(device))\n",
    "    return loss, acc\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 4. SPSA for Logistic Regression\n",
    "# ---------------------------\n",
    "\n",
    "def spsa_logistic_train(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    max_iters=200,\n",
    "    a=0.1,\n",
    "    c=0.1,\n",
    "    alpha=0.602,\n",
    "    gamma=0.101,\n",
    "    A=10.0,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    \"\"\"\n",
    "    SPSA training for logistic regression:\n",
    "        theta includes weights and bias as a single vector.\n",
    "    Uses full-batch loss for simplicity (n is small).\n",
    "\n",
    "    Returns:\n",
    "        theta  : final parameter vector (weights + bias)\n",
    "        history: dict with loss & accuracy over iterations.\n",
    "    \"\"\"\n",
    "    X_train = X_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "    X_val = X_val.to(device)\n",
    "    y_val = y_val.to(device)\n",
    "\n",
    "    n, d = X_train.shape\n",
    "    # Initialize parameters near zero\n",
    "    theta = torch.zeros(d + 1, dtype=torch.float32, device=device)\n",
    "\n",
    "    def loss_fn(theta, X, y):\n",
    "        w = theta[:-1]\n",
    "        b = theta[-1]\n",
    "        logits = X @ w + b\n",
    "        probs = torch.sigmoid(logits)\n",
    "        eps = 1e-8\n",
    "        loss = - (y * torch.log(probs + eps) +\n",
    "                  (1 - y) * torch.log(1 - probs + eps)).mean()\n",
    "        return loss\n",
    "\n",
    "    def acc_fn(theta, X, y):\n",
    "        w = theta[:-1]\n",
    "        b = theta[-1]\n",
    "        logits = X @ w + b\n",
    "        probs = torch.sigmoid(logits)\n",
    "        y_pred = (probs >= 0.5).long()\n",
    "        return (y_pred == y).float().mean().item()\n",
    "\n",
    "    history = {\"iter\": [], \"train_loss\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "    for k in range(max_iters):\n",
    "        ak = a / ((k + 1 + A) ** alpha)\n",
    "        ck = c / ((k + 1) ** gamma)\n",
    "\n",
    "        # Rademacher perturbation: +/- 1\n",
    "        delta = torch.randint(low=0, high=2, size=theta.shape, device=device) * 2 - 1\n",
    "        theta_plus = theta + ck * delta\n",
    "        theta_minus = theta - ck * delta\n",
    "\n",
    "        # Evaluate loss at perturbed points\n",
    "        J_plus = loss_fn(theta_plus, X_train, y_train)\n",
    "        J_minus = loss_fn(theta_minus, X_train, y_train)\n",
    "\n",
    "        # Estimate gradient\n",
    "        g_hat = (J_plus - J_minus) / (2.0 * ck) * delta  # element-wise\n",
    "\n",
    "        # Update\n",
    "        theta = theta - ak * g_hat\n",
    "\n",
    "        # Logging\n",
    "        if (k + 1) % 10 == 0 or k == 0:\n",
    "            train_loss = loss_fn(theta, X_train, y_train).item()\n",
    "            val_loss = loss_fn(theta, X_val, y_val).item()\n",
    "            val_acc = acc_fn(theta, X_val, y_val)\n",
    "\n",
    "            history[\"iter\"].append(k + 1)\n",
    "            history[\"train_loss\"].append(train_loss)\n",
    "            history[\"val_loss\"].append(val_loss)\n",
    "            history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "            print(\n",
    "                f\"[SPSA] Iter {k+1:4d} | \"\n",
    "                f\"Train Loss: {train_loss:.4f} | \"\n",
    "                f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\"\n",
    "            )\n",
    "\n",
    "    return theta, history\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 4b. SPSA for MLP\n",
    "# ---------------------------\n",
    "\n",
    "def spsa_mlp_train(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    input_dim,\n",
    "    hidden_dim=32,\n",
    "    max_iters=200,\n",
    "    a=0.05,\n",
    "    c=0.05,\n",
    "    alpha=0.602,\n",
    "    gamma=0.101,\n",
    "    A=10.0,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    \"\"\"\n",
    "    SPSA training for the SmallMLP model.\n",
    "    We flatten all parameters into a single vector theta, perturb it,\n",
    "    compute loss via forward passes, and update theta via SPSA.\n",
    "    \"\"\"\n",
    "    X_train = X_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "    X_val = X_val.to(device)\n",
    "    y_val = y_val.to(device)\n",
    "\n",
    "    model = SmallMLP(input_dim, hidden_dim=hidden_dim).to(device)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # --- helpers to flatten / unflatten parameters ---\n",
    "\n",
    "    params = list(model.parameters())\n",
    "    shapes = [p.shape for p in params]\n",
    "    sizes = [p.numel() for p in params]\n",
    "    cum_sizes = np.cumsum([0] + sizes)\n",
    "\n",
    "    def flatten_params():\n",
    "        return torch.cat([p.detach().view(-1) for p in params])\n",
    "\n",
    "    def set_params(theta_vec):\n",
    "        with torch.no_grad():\n",
    "            for i, p in enumerate(params):\n",
    "                start = cum_sizes[i]\n",
    "                end = cum_sizes[i + 1]\n",
    "                p.copy_(theta_vec[start:end].view(shapes[i]))\n",
    "\n",
    "    theta = flatten_params().to(device)\n",
    "\n",
    "    def loss_for_theta(theta_vec, X, y):\n",
    "        set_params(theta_vec)\n",
    "        with torch.no_grad():\n",
    "            logits = model(X)\n",
    "            loss = loss_fn(logits, y.float())\n",
    "        return loss\n",
    "\n",
    "    def acc_for_theta(theta_vec, X, y):\n",
    "        set_params(theta_vec)\n",
    "        with torch.no_grad():\n",
    "            logits = model(X)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            y_pred = (probs >= 0.5).long()\n",
    "            acc = (y_pred == y).float().mean().item()\n",
    "        return acc\n",
    "\n",
    "    history = {\"iter\": [], \"train_loss\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "    for k in range(max_iters):\n",
    "        ak = a / ((k + 1 + A) ** alpha)\n",
    "        ck = c / ((k + 1) ** gamma)\n",
    "\n",
    "        delta = torch.randint(low=0, high=2, size=theta.shape, device=device) * 2 - 1\n",
    "        theta_plus = theta + ck * delta\n",
    "        theta_minus = theta - ck * delta\n",
    "\n",
    "        J_plus = loss_for_theta(theta_plus, X_train, y_train)\n",
    "        J_minus = loss_for_theta(theta_minus, X_train, y_train)\n",
    "\n",
    "        g_hat = (J_plus - J_minus) / (2.0 * ck) * delta\n",
    "        theta = theta - ak * g_hat\n",
    "\n",
    "        if (k + 1) % 10 == 0 or k == 0:\n",
    "            train_loss = loss_for_theta(theta, X_train, y_train).item()\n",
    "            val_loss = loss_for_theta(theta, X_val, y_val).item()\n",
    "            val_acc = acc_for_theta(theta, X_val, y_val)\n",
    "            history[\"iter\"].append(k + 1)\n",
    "            history[\"train_loss\"].append(train_loss)\n",
    "            history[\"val_loss\"].append(val_loss)\n",
    "            history[\"val_acc\"].append(val_acc)\n",
    "            print(\n",
    "                f\"[MLP SPSA] Iter {k+1:4d} | \"\n",
    "                f\"Train Loss: {train_loss:.4f} | \"\n",
    "                f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\"\n",
    "            )\n",
    "\n",
    "    # set final params into model before returning\n",
    "    set_params(theta)\n",
    "    return model, history\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Experiment runners\n",
    "# ---------------------------\n",
    "\n",
    "def run_logistic_experiments(device=\"cpu\"):\n",
    "    print(\"=== Logistic Regression Experiments ===\")\n",
    "    X_train, X_test, y_train, y_test = load_breast_cancer_torch(device=device)\n",
    "\n",
    "    # Train/val split on training set (e.g. 80/20)\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "        X_train.cpu().numpy(),\n",
    "        y_train.cpu().numpy(),\n",
    "        test_size=0.2,\n",
    "        random_state=0,\n",
    "        stratify=y_train.cpu().numpy()\n",
    "    )\n",
    "    X_tr = torch.tensor(X_tr, dtype=torch.float32, device=device)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32, device=device)\n",
    "    y_tr = torch.tensor(y_tr, dtype=torch.long, device=device)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.long, device=device)\n",
    "\n",
    "    input_dim = X_tr.shape[1]\n",
    "    batch_size_sgd = 32\n",
    "    n_epochs = 50\n",
    "\n",
    "    # Loss function\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # DataLoaders\n",
    "    full_train_ds = TensorDataset(X_tr, y_tr)\n",
    "    full_train_loader = DataLoader(full_train_ds, batch_size=len(full_train_ds), shuffle=True)\n",
    "    sgd_train_loader = DataLoader(full_train_ds, batch_size=batch_size_sgd, shuffle=True)\n",
    "\n",
    "    # Store results\n",
    "    results = {}\n",
    "\n",
    "    # ---- Gradient Descent (full batch) ----\n",
    "    model_gd = LogisticModel(input_dim).to(device)\n",
    "    optimizer_gd = torch.optim.SGD(model_gd.parameters(), lr=0.1)\n",
    "    print(\"\\n[Logistic] Training with Gradient Descent (full batch)...\")\n",
    "    start = time.time()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_loss = train_one_epoch(model_gd, optimizer_gd, loss_fn, full_train_loader, device)\n",
    "        val_loss, val_acc = evaluate_model(model_gd, loss_fn, X_val, y_val, device)\n",
    "        if epoch % 10 == 0 or epoch == 1:\n",
    "            print(\n",
    "                f\"Epoch {epoch:3d} | Train Loss: {train_loss:.4f} | \"\n",
    "                f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\"\n",
    "            )\n",
    "    elapsed_gd = time.time() - start\n",
    "    test_loss, test_acc = evaluate_model(model_gd, loss_fn, X_test, y_test, device)\n",
    "    results[\"GD\"] = (test_loss, test_acc, elapsed_gd)\n",
    "\n",
    "    # ---- SGD (mini-batch) ----\n",
    "    model_sgd = LogisticModel(input_dim).to(device)\n",
    "    optimizer_sgd = torch.optim.SGD(model_sgd.parameters(), lr=0.05)\n",
    "    print(\"\\n[Logistic] Training with SGD (mini-batch)...\")\n",
    "    start = time.time()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_loss = train_one_epoch(model_sgd, optimizer_sgd, loss_fn, sgd_train_loader, device)\n",
    "        val_loss, val_acc = evaluate_model(model_sgd, loss_fn, X_val, y_val, device)\n",
    "        if epoch % 10 == 0 or epoch == 1:\n",
    "            print(\n",
    "                f\"Epoch {epoch:3d} | Train Loss: {train_loss:.4f} | \"\n",
    "                f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\"\n",
    "            )\n",
    "    elapsed_sgd = time.time() - start\n",
    "    test_loss, test_acc = evaluate_model(model_sgd, loss_fn, X_test, y_test, device)\n",
    "    results[\"SGD\"] = (test_loss, test_acc, elapsed_sgd)\n",
    "\n",
    "    # ---- Nesterov Accelerated Gradient (NAG) ----\n",
    "    model_nag = LogisticModel(input_dim).to(device)\n",
    "    optimizer_nag = torch.optim.SGD(model_nag.parameters(), lr=0.05, momentum=0.9, nesterov=True)\n",
    "    print(\"\\n[Logistic] Training with Nesterov Accelerated Gradient...\")\n",
    "    start = time.time()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_loss = train_one_epoch(model_nag, optimizer_nag, loss_fn, sgd_train_loader, device)\n",
    "        val_loss, val_acc = evaluate_model(model_nag, loss_fn, X_val, y_val, device)\n",
    "        if epoch % 10 == 0 or epoch == 1:\n",
    "            print(\n",
    "                f\"Epoch {epoch:3d} | Train Loss: {train_loss:.4f} | \"\n",
    "                f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\"\n",
    "            )\n",
    "    elapsed_nag = time.time() - start\n",
    "    test_loss, test_acc = evaluate_model(model_nag, loss_fn, X_test, y_test, device)\n",
    "    results[\"NAG\"] = (test_loss, test_acc, elapsed_nag)\n",
    "\n",
    "    # ---- Adam ----\n",
    "    model_adam = LogisticModel(input_dim).to(device)\n",
    "    optimizer_adam = torch.optim.Adam(model_adam.parameters(), lr=0.01)\n",
    "    print(\"\\n[Logistic] Training with Adam...\")\n",
    "    start = time.time()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_loss = train_one_epoch(model_adam, optimizer_adam, loss_fn, sgd_train_loader, device)\n",
    "        val_loss, val_acc = evaluate_model(model_adam, loss_fn, X_val, y_val, device)\n",
    "        if epoch % 10 == 0 or epoch == 1:\n",
    "            print(\n",
    "                f\"Epoch {epoch:3d} | Train Loss: {train_loss:.4f} | \"\n",
    "                f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\"\n",
    "            )\n",
    "    elapsed_adam = time.time() - start\n",
    "    test_loss, test_acc = evaluate_model(model_adam, loss_fn, X_test, y_test, device)\n",
    "    results[\"Adam\"] = (test_loss, test_acc, elapsed_adam)\n",
    "\n",
    "    # ---- SPSA ----\n",
    "    print(\"\\n[Logistic] Training with SPSA (gradient-free)...\")\n",
    "    start = time.time()\n",
    "    theta_spsa, spsa_hist = spsa_logistic_train(\n",
    "        X_tr, y_tr, X_val, y_val,\n",
    "        max_iters=200,\n",
    "        a=0.1,\n",
    "        c=0.1,\n",
    "        alpha=0.602,\n",
    "        gamma=0.101,\n",
    "        A=10.0,\n",
    "        device=device\n",
    "    )\n",
    "    elapsed_spsa = time.time() - start\n",
    "\n",
    "    def spsa_loss_acc(theta, X, y):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        w = theta[:-1]\n",
    "        b = theta[-1]\n",
    "        logits = X @ w + b\n",
    "        probs = torch.sigmoid(logits)\n",
    "        eps = 1e-8\n",
    "        loss = - (y * torch.log(probs + eps) +\n",
    "                  (1 - y) * torch.log(1 - probs + eps)).mean().item()\n",
    "        y_pred = (probs >= 0.5).long()\n",
    "        acc = (y_pred == y).float().mean().item()\n",
    "        return loss, acc\n",
    "\n",
    "    test_loss_spsa, test_acc_spsa = spsa_loss_acc(theta_spsa, X_test, y_test)\n",
    "    results[\"SPSA\"] = (test_loss_spsa, test_acc_spsa, elapsed_spsa)\n",
    "    print(f\"\\n[SPSA] Finished in {elapsed_spsa:.3f} seconds.\")\n",
    "\n",
    "    # Print summary table\n",
    "    print(\"\\n=== Logistic Regression Summary (Test Set) ===\")\n",
    "    print(f\"{'Method':8s} | {'Test Loss':>10s} | {'Test Acc':>9s} | {'Time (s)':>8s}\")\n",
    "    print(\"-\" * 45)\n",
    "    for method, (loss, acc, t) in results.items():\n",
    "        print(f\"{method:8s} | {loss:10.4f} | {acc:9.4f} | {t:8.3f}\")\n",
    "\n",
    "\n",
    "def run_mlp_experiments(device=\"cpu\"):\n",
    "    print(\"\\n\\n=== Small Neural Network Experiments ===\")\n",
    "    X_train, X_test, y_train, y_test = load_breast_cancer_torch(device=device)\n",
    "\n",
    "    # Train/val split on training set\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "        X_train.cpu().numpy(),\n",
    "        y_train.cpu().numpy(),\n",
    "        test_size=0.2,\n",
    "        random_state=0,\n",
    "        stratify=y_train.cpu().numpy()\n",
    "    )\n",
    "    X_tr = torch.tensor(X_tr, dtype=torch.float32, device=device)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32, device=device)\n",
    "    y_tr = torch.tensor(y_tr, dtype=torch.long, device=device)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.long, device=device)\n",
    "\n",
    "    input_dim = X_tr.shape[1]\n",
    "    batch_size = 32\n",
    "    n_epochs = 20  # smaller, NN is more expensive than logistic\n",
    "\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    train_ds = TensorDataset(X_tr, y_tr)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # ---- GD (full batch) ----\n",
    "    model_gd = SmallMLP(input_dim, hidden_dim=32).to(device)\n",
    "    optimizer_gd = torch.optim.SGD(model_gd.parameters(), lr=0.1)\n",
    "    full_train_loader = DataLoader(train_ds, batch_size=len(train_ds), shuffle=True)\n",
    "\n",
    "    print(\"\\n[MLP] Training with Gradient Descent (full batch)...\")\n",
    "    start = time.time()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_loss = train_one_epoch(model_gd, optimizer_gd, loss_fn, full_train_loader, device)\n",
    "        val_loss, val_acc = evaluate_model(model_gd, loss_fn, X_val, y_val, device)\n",
    "        if epoch % 5 == 0 or epoch == 1:\n",
    "            print(\n",
    "                f\"Epoch {epoch:3d} | Train Loss: {train_loss:.4f} | \"\n",
    "                f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\"\n",
    "            )\n",
    "    elapsed_gd = time.time() - start\n",
    "    test_loss, test_acc = evaluate_model(model_gd, loss_fn, X_test, y_test, device)\n",
    "    results[\"GD\"] = (test_loss, test_acc, elapsed_gd)\n",
    "\n",
    "    # ---- SGD ----\n",
    "    model_sgd = SmallMLP(input_dim, hidden_dim=32).to(device)\n",
    "    optimizer_sgd = torch.optim.SGD(model_sgd.parameters(), lr=0.05)\n",
    "    print(\"\\n[MLP] Training with SGD (mini-batch)...\")\n",
    "    start = time.time()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_loss = train_one_epoch(model_sgd, optimizer_sgd, loss_fn, train_loader, device)\n",
    "        val_loss, val_acc = evaluate_model(model_sgd, loss_fn, X_val, y_val, device)\n",
    "        if epoch % 5 == 0 or epoch == 1:\n",
    "            print(\n",
    "                f\"Epoch {epoch:3d} | Train Loss: {train_loss:.4f} | \"\n",
    "                f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\"\n",
    "            )\n",
    "    elapsed_sgd = time.time() - start\n",
    "    test_loss, test_acc = evaluate_model(model_sgd, loss_fn, X_test, y_test, device)\n",
    "    results[\"SGD\"] = (test_loss, test_acc, elapsed_sgd)\n",
    "\n",
    "    # ---- NAG ----\n",
    "    model_nag = SmallMLP(input_dim, hidden_dim=32).to(device)\n",
    "    optimizer_nag = torch.optim.SGD(model_nag.parameters(), lr=0.05, momentum=0.9, nesterov=True)\n",
    "    print(\"\\n[MLP] Training with Nesterov Accelerated Gradient...\")\n",
    "    start = time.time()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_loss = train_one_epoch(model_nag, optimizer_nag, loss_fn, train_loader, device)\n",
    "        val_loss, val_acc = evaluate_model(model_nag, loss_fn, X_val, y_val, device)\n",
    "        if epoch % 5 == 0 or epoch == 1:\n",
    "            print(\n",
    "                f\"Epoch {epoch:3d} | Train Loss: {train_loss:.4f} | \"\n",
    "                f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\"\n",
    "            )\n",
    "    elapsed_nag = time.time() - start\n",
    "    test_loss, test_acc = evaluate_model(model_nag, loss_fn, X_test, y_test, device)\n",
    "    results[\"NAG\"] = (test_loss, test_acc, elapsed_nag)\n",
    "\n",
    "    # ---- Adam ----\n",
    "    model_adam = SmallMLP(input_dim, hidden_dim=32).to(device)\n",
    "    optimizer_adam = torch.optim.Adam(model_adam.parameters(), lr=0.01)\n",
    "    print(\"\\n[MLP] Training with Adam...\")\n",
    "    start = time.time()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_loss = train_one_epoch(model_adam, optimizer_adam, loss_fn, train_loader, device)\n",
    "        val_loss, val_acc = evaluate_model(model_adam, loss_fn, X_val, y_val, device)\n",
    "        if epoch % 5 == 0 or epoch == 1:\n",
    "            print(\n",
    "                f\"Epoch {epoch:3d} | Train Loss: {train_loss:.4f} | \"\n",
    "                f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\"\n",
    "            )\n",
    "    elapsed_adam = time.time() - start\n",
    "    test_loss, test_acc = evaluate_model(model_adam, loss_fn, X_test, y_test, device)\n",
    "    results[\"Adam\"] = (test_loss, test_acc, elapsed_adam)\n",
    "\n",
    "    # ---- SPSA for MLP ----\n",
    "    print(\"\\n[MLP] Training with SPSA (gradient-free)...\")\n",
    "    start = time.time()\n",
    "    model_spsa, spsa_hist_mlp = spsa_mlp_train(\n",
    "        X_tr, y_tr, X_val, y_val,\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim=32,\n",
    "        max_iters=200,\n",
    "        a=0.05,\n",
    "        c=0.05,\n",
    "        alpha=0.602,\n",
    "        gamma=0.101,\n",
    "        A=10.0,\n",
    "        device=device\n",
    "    )\n",
    "    elapsed_spsa_mlp = time.time() - start\n",
    "    test_loss_spsa, test_acc_spsa = evaluate_model(model_spsa, loss_fn, X_test, y_test, device)\n",
    "    results[\"SPSA\"] = (test_loss_spsa, test_acc_spsa, elapsed_spsa_mlp)\n",
    "\n",
    "    # Summary table\n",
    "    print(\"\\n=== MLP Summary (Test Set) ===\")\n",
    "    print(f\"{'Method':8s} | {'Test Loss':>10s} | {'Test Acc':>9s} | {'Time (s)':>8s}\")\n",
    "    print(\"-\" * 45)\n",
    "    for method, (loss, acc, t) in results.items():\n",
    "        print(f\"{method:8s} | {loss:10.4f} | {acc:9.4f} | {t:8.3f}\")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Main\n",
    "# ---------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    run_logistic_experiments(device=device)\n",
    "    run_mlp_experiments(device=device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (torch310 GPU)",
   "language": "python",
   "name": "torch310_mini"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
